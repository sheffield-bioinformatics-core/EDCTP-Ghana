import os.path
from os import path
import glob
import csv

samples={}
with open("samples.csv") as fd:
    rd = csv.reader(fd, delimiter=",", quotechar='"')
    for row in rd:
        if len(row)>0:
            if row[0] != "sample":
                samples[row[0]]=row[1]



## find run folder
run_folder = glob.glob("/data/"+config["user_run_name"]+"/*/*")[0]
run_name = os.path.basename(run_folder)
## find sequencing summary
sequencing_summary = glob.glob(run_folder+"/sequencing_summary*.txt")[0]

## hardcode some stuff
code_path = '/home/sarscov2/wc/EDCTP-Ghana/workflow'
scheme_dir = '/home/sarscov2/artic-ncov2019/primer_schemes'
artic_version = 'v1.1.3'
primers= 'V3'


## what if a sample fails, we need to control for no reads - here we delete it from the dictionary

for sample in samples.keys():
    to_delete=[]
    barcode = samples[sample]
    barcode_path = run_folder+"/fastq_pass/"+barcode
    print(barcode_path)
    if not path.exists(barcode_path):
        to_delete.append(sample)
if len(to_delete) > 0:
    for sample in to_delete:
        samples.pop(sample,None)


rule all:
    input:
        expand("{run_name}_{barcode}.fastq",run_name=run_name,barcode=samples.values()),
        expand("{sample}.consensus.fasta",sample=samples),
        expand("{run_name}-consensus.gisaid.fasta",run_name=run_name),
        expand("{run_name}_pangolin_lineage_report.csv",run_name=run_name),
        expand("{sample}.primertrimmed.rg.sorted.bam.depth_region",sample=samples),
        expand("{sample}.consensus.gisaid.fasta",sample=samples),
        expand("{sample}.consensus.gisaid.fasta.non_n_count",sample=samples),
        expand("{sample}.sorted.mapped.bam",sample=samples),
        expand("{sample}.upload.done",sample=samples),
        "fastq.done",
        "reports.done",
        "samples.done",
        "pangolin.done",
        "post.done"

        #"pipeline.done"

rule upload_fastq:
    input:
        fastq_pass=run_folder+"/fastq_pass"
    output:
        "fastq.done"
    params:
        run_name=run_name,
    shell:
         "rclone copy {input.fastq_pass} edctp-ghana:RESULTS/{params.run_name}/RAW_FASTQ && touch fastq.done"

rule upload_report:
    input:
        run_folder=run_folder
    output:
        "reports.done"
    params:
        run_name=run_name,
    shell:
         "rclone copy {input.run_folder}/report* edctp-ghana:RESULTS/{params.run_name}/REPORTS/ && touch reports.done"

rule upload_samples:
    output:
        "samples.done"
    params:
        run_name=run_name,
    shell:
         "rclone copy samples.csv edctp-ghana:RESULTS/{params.run_name}/ && touch samples.done"


rule guppylex:
    output:
        "{run_name}_{barcode}.fastq"
    params:
        run_name=run_name,
        fastq_pass=run_folder+"/fastq_pass"
    conda:
        "envs/environment_artic.yml"
    shell:
        "artic guppyplex --min-length 400 --skip-quality-check --max-length 700 --directory {params.fastq_pass}/{wildcards.barcode} --prefix {params.run_name}  || touch {params.run_name}_{wildcards.barcode}.fastq"

rule artic:
    input:
        fastq = lambda wildcards: run_name+"_"+samples[wildcards.sample]+".fastq"
    output:
        "{sample}.consensus.fasta",
        "{sample}.primertrimmed.rg.sorted.bam",
        "{sample}.sorted.bam",
        "{sample}-barplot.png"
    params:
        summary = sequencing_summary,
        scheme_dir = scheme_dir,
        fast5_dir = lambda wildcards: run_folder+"/fast5_pass/"+samples[wildcards.sample],
        primers =primers
    conda:
        "envs/environment_artic.yml"
    shell:
        "artic minion --normalise 200 --threads 4 --scheme-directory {params.scheme_dir} --read-file {input.fastq} --fast5-directory {params.fast5_dir}  --sequencing-summary {params.summary} nCoV-2019/{params.primers} {wildcards.sample} || touch {wildcards.sample}.consensus.fasta {wildcards.sample}.primertrimmed.rg.sorted.bam {wildcards.sample}.sorted.bam"

rule sambamba:
    input:
        "{sample}.primertrimmed.rg.sorted.bam"
    output:
        "{sample}.primertrimmed.rg.sorted.bam.depth_region"
    params:
        bed = code_path+"/resources/amplicon_regions_"+primers+".bed",
    shell:
       "sambamba depth region -F 'mapping_quality > 0' -o {output} -L {params.bed} {input} || touch {wildcards.sample}.primertrimmed.rg.sorted.bam.depth_region"

rule dissemination:
    input:
        bam = "{sample}.sorted.bam",
        fasta = "{sample}.consensus.fasta"
    output:
        "{sample}.sorted.mapped.bam",
        "{sample}.consensus.gisaid.fasta"
    params:
        dissemination = expand("{code_path}/bin/dissemination.sh",code_path=code_path),
    shell:
        "bash {params.dissemination} {wildcards.sample} {input.bam} {input.fasta}"

rule get_non_n_count:
    input:
        fasta = "{sample}.consensus.gisaid.fasta"
    output:
        "{sample}.consensus.gisaid.fasta.non_n_count"
    shell:
        "grep -v '>' {input.fasta} | tr -d 'N' | tr -d '\n' | perl -pe 's/\s+//g' | wc -m > {output} || touch {sample}.consensus.fasta.non_n_count"

rule merge_fasta:
    input:
        fasta = expand("{sample}.consensus.gisaid.fasta", sample=samples.keys()),
        non_n_count = expand("{sample}.consensus.gisaid.fasta.non_n_count", sample=samples.keys())
    output:
         expand("{run_name}-consensus.gisaid.fasta",run_name=run_name)
    params:
        run_name=run_name
    shell:
        """
        for fasta in {input.fasta}
          do non_n_count=`cat $fasta.non_n_count`
          if [ $non_n_count -ge 14935 ]
            then cat $fasta >> {params.run_name}-consensus.gisaid.fasta
          else
            # Ensure output file exists even if this condition is never met
            touch {params.run_name}-consensus.gisaid.fasta
          fi;
        done
        """

rule pangolin:
    input:
        fasta=expand("{run_name}-consensus.gisaid.fasta",run_name=run_name)
    output:
        run_name+"_pangolin_lineage_report.csv"
    conda:
        "envs/environment_pangolin.yml"
    shell:
        "pangolin {input.fasta} --outfile {output}  || touch {output}"

rule upload_pangolin:
    input:
        run_name+"_pangolin_lineage_report.csv"
    output:
        "pangolin.done"
    params:
        run_name = run_name
    shell:
        "rclone copy {input} edctp-ghana:RESULTS/{params.run_name}/ && touch pangolin.done"

rule upload:
    input:
        fasta = lambda wildcards: wildcards.sample+".consensus.fasta",
        bam = lambda wildcards: wildcards.sample+".sorted.mapped.bam",
        png = lambda wildcards: wildcards.sample+"-barplot.png"
    output:
        "{sample}.upload.done"
    params:
        run_name=run_name
    shell:
         "rclone copy {input.bam} edctp-ghana:RESULTS/{params.run_name}/ && rclone copy {input.fasta} edctp-ghana:RESULTS/{params.run_name}/ &&  rclone copy {input.png} edctp-ghana:RESULTS/{params.run_name}/ && touch {wildcards.sample}.upload.done || touch {wildcards.sample}.upload.done "


rule post_to_slack:
    input:
        "pangolin.done"
    output:
        "post.done"
    params:
        post_process = expand("{code_path}/bin/post_process.py",code_path=code_path),
        run_name = run_name
    shell:
        "python {params.post_process} --run-id {params.run_name} && touch post.done"


########################################################################################################
## HERE LIES DANGER - WE WILL RSYNC TO A REMOTE VOLUME AND DELETE THE LOCAL COPY "--remove-source-files"
########################################################################################################